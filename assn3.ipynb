{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This methods accepts:\n",
    "    data - a list of words\n",
    "    labels - a list of tags (as #s) corresponding to data (defaults to none for test data)\n",
    "    tag_delim - the tag corresponding to -DOCSTART- that we split the tags on\n",
    "    word_delim - the word to split the sentences on\n",
    "Returns:\n",
    "    sentences: a list of lists. Each sublist holds the words of a given sentence\n",
    "    sentence_labels: a list of lists of the corresponding tags (as #s) to the sentences\n",
    "    \n",
    "NOTE: This is called within read_data\n",
    "'''\n",
    "\n",
    "def get_sentences(data, labels = None, tag_delim = 0, word_delim = '-DOCSTART-'):\n",
    "    sentences = []\n",
    "    for x, y in itertools.groupby(data, lambda z: z == word_delim):\n",
    "        if x: sentences.append([])\n",
    "        sentences[-1].extend(y)\n",
    "    if labels is None:\n",
    "        return sentences\n",
    "    else:\n",
    "        sentence_labels = []\n",
    "        for x, y in itertools.groupby(labels, lambda z: z == tag_delim):\n",
    "            if x: sentence_labels.append([])\n",
    "            sentence_labels[-1].extend(y)\n",
    "        return sentences, sentence_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This method accepts:\n",
    "    data_file and label_file (optional) - file names for words and corresponding tags\n",
    "Returns:\n",
    "    sentences - a list of sentences, where each sentence is represented as a list of words and begins with -DOCSTART-\n",
    "    sentences_tags - a list of lists of tags corresponding to the sentences, where tags are represented as integers\n",
    "    tag_list - a list of the unique tags. The index of each tag is what we replace all tags with. \n",
    "            Later, we will use this list to convert number tags back to actual tags:\n",
    "            tags = [tag_list[x] for x in tags]\n",
    "            \n",
    "'''\n",
    "def read_data(data_file, label_file = None):\n",
    "    data = pd.read_csv(data_file)\n",
    "    del data['id'] \n",
    "    if label_file is None:\n",
    "        return get_sentences(list(data['word']))\n",
    "    else:\n",
    "        labels = pd.read_csv(label_file)\n",
    "        del labels['id']\n",
    "        \n",
    "        # convert labels to numbers and store the conversion from # back to tag in a dictionary tag_list\n",
    "        labels['tag'] = labels['tag'].astype('category')\n",
    "        tag_list = list(labels['tag'].cat.categories)\n",
    "        docstart_tag = tag_list.index('O') # get # corresponding to tag 'O'\n",
    "        labels = np.array(labels['tag'].cat.codes)\n",
    "\n",
    "        sentences, sentences_tags = get_sentences(list(data['word']), labels, tag_delim = docstart_tag)\n",
    "        return sentences, sentences_tags, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how we call read_data with all the files:\n",
    "# DIEGO: Make sure that in read_data. dev and train  have the same tag_list distribution. They can't have different tag numbers.\n",
    "dev_x, dev_y, tag_list = read_data('data/dev_x.csv', 'data/dev_y.csv')\n",
    "train_x, train_y, tag_list = read_data('data/train_x.csv', 'data/train_y.csv')\n",
    "test_x = read_data('data/test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.emissions = None\n",
    "        self.transitions = None\n",
    "    \n",
    "    '''\n",
    "    return all emissions (for all words,tags)\n",
    "    this is structured as a dictionary where each key is a word and it's value is a numpy array with a value for each tag\n",
    "    To get e(word|tag), you do emissions[word][tag]\n",
    "    '''\n",
    "    def getEmissions(self):\n",
    "        return self.emissions\n",
    "\n",
    "    '''returns emissions for a given word as a numpy array where each value corresponds to a tag => e(x|y) for all y values'''\n",
    "    def getEmissionsByWord(self, word):\n",
    "        return self.emissions[word]\n",
    "\n",
    "    '''\n",
    "    returns all transitions(for all states)\n",
    "    this is structured as a dictionary where each key is a previous state and it's value is a numpy array with a value for each tag\n",
    "    To get q(tag|prev_state), you do transitions[prev_state][tag]\n",
    "    Prev_state is the previous tag in the bigram case, and previous 2 tags joined with ',' in trigram case\n",
    "    '''\n",
    "    def getTransitions(self):\n",
    "        return self.transitions\n",
    "   \n",
    "    '''\n",
    "    same as getTransitions but returns all transitions for a given state.\n",
    "    '''\n",
    "    def getTransitionsByState(self, prev):\n",
    "        return self.transitions[prev]\n",
    "\n",
    "    '''\n",
    "    prev = previous state as string\n",
    "    curr = current state (tag)\n",
    "    returns q(curr|prev)\n",
    "    '''\n",
    "    def getTransition(self, prev, curr):\n",
    "        return self.transitions[prev][curr]\n",
    "    \n",
    "    '''\n",
    "    This method accepts:\n",
    "        sentences - list of sentences, each represented as a list of words\n",
    "        sentences_tags - tags (as #s) corresponding to the words in sentences\n",
    "        tag_list - list of unique tags\n",
    "        n_prev - # of previous tags to consider. 2 for trigram, 1 for bigram\n",
    "    This calculates the transitions and emissions and stores them in the member variables\n",
    "    '''\n",
    "    def process(self, sentences, sentences_tags, tag_list, n_prev):\n",
    "        num_tags = len(tag_list)\n",
    "        self.__calculate_transitions(sentences_tags, num_tags, n_prev)\n",
    "        self.__calculate_emissions(sentences, sentences_tags, num_tags)\n",
    "        return\n",
    "    \n",
    "    '''\n",
    "    Given the tags for each sentence and the number of unique tags,\n",
    "    this method returns a dictionary where key = tag and value = tag's count \n",
    "    '''\n",
    "    def __calculate_tag_counts(self, sentences_tags, num_tags):\n",
    "        tag_counts = [0] * num_tags\n",
    "        for sent_tags in sentences_tags:\n",
    "            for tag in sent_tags:\n",
    "                tag_counts[tag] +=1\n",
    "        return tag_counts\n",
    "    \n",
    "    '''\n",
    "    This method, called in process, calculates the emissions, \n",
    "    structured as a dictionary where each key is a word and it's value is a numpy array with a value for each tag.\n",
    "    To get e(word|tag), you do e[word][tag]\n",
    "    Given: list of sentences, corresponding tags, and number of unique tags\n",
    "    Emissions are calculated as the log((count(word, tag) + 1) / (count(tag) + ?????)) - using add-1 smoothing\n",
    "    DIEGO: ???? should be the size of dictionary of words.\n",
    "    \n",
    "    ****************\n",
    "    QUESTION - If I'm starting all count(word,tag) at 1, do I also need to be doing that for count(tag)\n",
    "        DIEGO: Yes. Otherwise you are providing inconsistency in the distribution. As I said, the counts should all start in zero\n",
    "        and the Add-1 should only be done when computing the log to get the probability distribution. It's much easier to track\n",
    "        the places where it's used and how it's used. You avoid adding 1 twice or none at all.\n",
    "    ***** Fix ^^^\n",
    "        DIEGO: FIXED\n",
    "    \n",
    "    '''\n",
    "    def __calculate_emissions(self, sentences, sentences_tags, num_tags):\n",
    "        e = {}\n",
    "        # DIEGO: This can be included within the first loop which would save an iteration over the entire set of tags.\n",
    "        tag_counts = self.__calculate_tag_counts(sentences_tags, num_tags) # get dictionary of tag counts\n",
    "        \n",
    "        # First go through all words/labels and count all (word,label) pairs\n",
    "        for sent, sent_tags in itertools.izip(sentences, sentences_tags):\n",
    "            for word, label in itertools.izip(sent, sent_tags):\n",
    "                if not word in e:\n",
    "                    e[word] = np.zeros(num_tags)\n",
    "                e[word][label] +=1   \n",
    "\n",
    "        dict_size = len(e)\n",
    "                \n",
    "        # Divide the counts(word,tag) by count(tag) and log it \n",
    "        for i in range(num_tags):\n",
    "            for word, value in e.iteritems():\n",
    "                 # We compute values using the log and also use add 1 smoothing\n",
    "                e[word][i] = np.log( (e[word][i] + 1) / (tag_counts[i] + dict_size))\n",
    "\n",
    "        # NOTE: STILL NEED TO TAKE CARE OF UNKNOWN WORDS BY DOING THE FREQUENCY THING!!!\n",
    "\n",
    "        del tag_counts\n",
    "        self.emissions = e\n",
    "        return\n",
    "    \n",
    "    '''\n",
    "    This method, called in process, calculates the transitions, \n",
    "    structured as a dictionary where each key is a previous state and it's value is a numpy array with a value for each tag.\n",
    "    To get q(tag|prev_state), you do transitions[prev_state][tag]\n",
    "    Prev_state is the previous tag in the bigram case, and previous 2 tags joined with ',' in trigram case\n",
    "    Given: \n",
    "        sentences_tags - list of tags separated by sentence, \n",
    "        num_tags - number of unique tags\n",
    "        n - number of previous states to use. 2 for trigram, 1 for bigrram\n",
    "    Transitions are calculated as the log((count(prev_state, tag) + 1) / (count(prev_state) + num_tags)) - using add-1 smoothing\n",
    "    '''\n",
    "    def __calculate_transitions(self, sentences_tags, num_tags, n):\n",
    "        q = {}\n",
    "        for state in itertools.product(range(-1,num_tags), repeat = n):\n",
    "            q[','.join(str(tag) for tag in state)] = np.zeros(num_tags)\n",
    "\n",
    "        for sent_tags in sentences_tags:  # for each sentence's list of tags\n",
    "            sent_tags = ([-1] * n) + sent_tags  #appending n '-1' tags to correspond to prior states/tags for the first word(s). \n",
    "\n",
    "            for i in range(len(sent_tags) - n):\n",
    "                prior_state = ','.join(str(sent_tags[j]) for j in range(i, i + n)) # Create a string with n sequenced tags\n",
    "                current_state = sent_tags[i + n]\n",
    "                q[prior_state][current_state] += 1\n",
    "\n",
    "        for prior_state, tag_list in q.iteritems():\n",
    "            q[prior_state] = np.log( (tag_list + 1) / (np.sum(tag_list) + num_tags))\n",
    "            \n",
    "        self.transitions = q   \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [['i', 'went','to','i'],['to','Diego'],['love', 'i']]\n",
    "sentences_tags = [[1,0,2,1],[2,3],[4,1]]\n",
    "num_tags = 5\n",
    "tag_list = [0,1,2,3,4]\n",
    "n_prev = 2 # 2 is for trigram. 1 is for bigram\n",
    "\n",
    "d.process(sentences, sentences_tags, tag_list, n_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getEmissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getEmissionsByWord('Diego')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getTransitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getTransition('1,0', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: TAKE CARE OF UNKNOWN WORDS AND SMOOTHING \n",
    "(FIX VITERBI TO GET RID OF -1s)\n",
    "Fix question that I wrote in calculate_emissions method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VITERBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = ['i', 'went','to','i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ac014a978e8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#VITERBI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;31m# trigram\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_tags' is not defined"
     ]
    }
   ],
   "source": [
    "#VITERBI\n",
    "n = 2 # trigram\n",
    "pis = [1] * num_tags\n",
    "paths = [[-1]*n] * num_tags\n",
    "\n",
    "for word in sentence:\n",
    "    e = d.getEmissionsByWord(word)\n",
    "    new_pis = [-float(\"inf\")] * num_tags\n",
    "    new_paths = [[]] * num_tags\n",
    "    for tag in range(num_tags):\n",
    "        e_pi = e[tag]\n",
    "        for prev_pi, prev_path in itertools.izip(pis, paths):\n",
    "            prev_state = ','.join(str(prev_path[j]) for j in range(-n, 0))\n",
    "            q_pi = d.getTransition(prev_state, tag)\n",
    "            pi = e_pi + prev_pi + q_pi\n",
    "            if pi > new_pis[tag]:\n",
    "                new_pis[tag] = pi\n",
    "                new_paths[tag] = prev_path + [tag]\n",
    "            \n",
    "    pis = new_pis\n",
    "    paths = new_paths\n",
    "\n",
    "\n",
    "best_path = paths[np.argmax(np.array(pis))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = Dictionary()\n",
    "d.process(train_x, train_y, tag_list, n_prev = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getEmissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getEmissionsByWord('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list[10] # the greates number in e for 'the' was in index 10, which is 'DT' --> makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getTransitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getTransition('1,0', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(sentence, d, num_tags = 45, n = 2):\n",
    "\n",
    "    pis = [1] * num_tags\n",
    "    paths = [[-1]*n] * num_tags\n",
    "\n",
    "    for word in sentence:\n",
    "        e = d.getEmissionsByWord(word)\n",
    "        new_pis = [-float(\"inf\")] * num_tags\n",
    "        new_paths = [[]] * num_tags\n",
    "        for tag in range(num_tags):\n",
    "            e_pi = e[tag]\n",
    "            for prev_pi, prev_path in itertools.izip(pis, paths):\n",
    "                prev_state = ','.join(str(prev_path[j]) for j in range(-n, 0))\n",
    "                q_pi = d.getTransition(prev_state, tag)\n",
    "                pi = e_pi + prev_pi + q_pi\n",
    "                if pi > new_pis[tag]:\n",
    "                    new_pis[tag] = pi\n",
    "                    new_paths[tag] = prev_path + [tag]\n",
    "\n",
    "        pis = new_pis\n",
    "        paths = new_paths\n",
    "\n",
    "\n",
    "    best_path = paths[np.argmax(np.array(pis))]\n",
    "    return best_path[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = viterbi(train_x[0], d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9375"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking current accuracy: 0.78 on train data -- not so great \n",
    "# need to fix the smoothing and unknown stuff\n",
    "np.sum(path[i] == train_y[0][i] for i in range(len(path)))/float(len(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getTransitions().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1387"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_x[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elem = dev_x[19]\n",
    "path = viterbi(elem, d)\n",
    "np.sum(path[i] == elem[i] for i in range(len(path)))/float(len(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
