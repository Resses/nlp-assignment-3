{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This methods accepts:\n",
    "    data - a list of words\n",
    "    labels - a list of tags (as #s) corresponding to data (defaults to none for test data)\n",
    "    tag_delim - the tag corresponding to -DOCSTART- that we split the tags on\n",
    "    word_delim - the word to split the sentences on\n",
    "Returns:\n",
    "    sentences: a list of lists. Each sublist holds the words of a given sentence\n",
    "    sentence_labels: a list of lists of the corresponding tags (as #s) to the sentences\n",
    "    \n",
    "NOTE: This is called within read_data\n",
    "'''\n",
    "\n",
    "def get_sentences(data, labels = None, tag_delim = 0, word_delim = '-DOCSTART-'):\n",
    "    sentences = []\n",
    "    for x, y in itertools.groupby(data, lambda z: z == word_delim):\n",
    "        if x: sentences.append([])\n",
    "        sentences[-1].extend(y)\n",
    "    if labels is None:\n",
    "        return sentences\n",
    "    else:\n",
    "        sentence_labels = []\n",
    "        for x, y in itertools.groupby(labels, lambda z: z == tag_delim):\n",
    "            if x: sentence_labels.append([])\n",
    "            sentence_labels[-1].extend(y)\n",
    "        return sentences, sentence_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This method accepts:\n",
    "    data_file and label_file (optional) - file names for words and corresponding tags\n",
    "Returns:\n",
    "    sentences - a list of sentences, where each sentence is represented as a list of words and begins with -DOCSTART-\n",
    "    sentences_tags - a list of lists of tags corresponding to the sentences, where tags are represented as integers\n",
    "    tag_list - a list of the unique tags. The index of each tag is what we replace all tags with. \n",
    "            Later, we will use this list to convert number tags back to actual tags:\n",
    "            tags = [tag_list[x] for x in tags]\n",
    "            \n",
    "'''\n",
    "def read_data(data_file, label_file = None):\n",
    "    data = pd.read_csv(data_file)\n",
    "    del data['id'] \n",
    "    if label_file is None:\n",
    "        return get_sentences(list(data['word']))\n",
    "    else:\n",
    "        labels = pd.read_csv(label_file)\n",
    "        del labels['id']\n",
    "        \n",
    "        # convert labels to numbers and store the conversion from # back to tag in a dictionary tag_list\n",
    "        labels['tag'] = labels['tag'].astype('category')\n",
    "        tag_list = list(labels['tag'].cat.categories)\n",
    "        docstart_tag = tag_list.index('O') # get # corresponding to tag 'O'\n",
    "        labels = np.array(labels['tag'].cat.codes)\n",
    "        sentences, sentences_tags = get_sentences(list(data['word']), labels, tag_delim = docstart_tag)\n",
    "        return sentences, sentences_tags, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how we call read_data with all the files:\n",
    "# DIEGO: Make sure that in read_data. dev and train  have the same tag_list distribution. They can't have different tag numbers.\n",
    "dev_x, dev_y, tag_list = read_data('data/dev_x.csv', 'data/dev_y.csv')\n",
    "train_x, train_y, tag_list = read_data('data/train_x.csv', 'data/train_y.csv')\n",
    "test_x = read_data('data/test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.emissions = None\n",
    "        self.transitions = None\n",
    "\n",
    "    '''\n",
    "    return all emissions (for all words,tags)\n",
    "    this is structured as a dictionary where each key is a word and it's value is a numpy array with a value for each tag\n",
    "    To get e(word|tag), you do emissions[word][tag]\n",
    "    '''\n",
    "    def getEmissions(self):\n",
    "        return self.emissions\n",
    "\n",
    "    '''returns emissions for a given word as a numpy array where each value corresponds to a tag => e(x|y) for all y values'''\n",
    "    def getEmissionsByWord(self, word):\n",
    "        return self.emissions[word]\n",
    "\n",
    "    ''' Determines if the word is in the dictionary '''\n",
    "    def isWordInDictionary(self, word):\n",
    "        return word in self.emissions\n",
    "\n",
    "    '''\n",
    "    returns all transitions(for all states)\n",
    "    this is structured as a dictionary where each key is a previous state and it's value is a numpy array with a value for each tag\n",
    "    To get q(tag|prev_state), you do transitions[prev_state][tag]\n",
    "    Prev_state is the previous tag in the bigram case, and previous 2 tags joined with ',' in trigram case\n",
    "    '''\n",
    "    def getTransitions(self):\n",
    "        return self.transitions\n",
    "\n",
    "    '''\n",
    "    same as getTransitions but returns all transitions for a given state.\n",
    "    '''\n",
    "    def getTransitionsByState(self, prev):\n",
    "        return self.transitions[prev]\n",
    "\n",
    "    '''\n",
    "    prev = previous state as string\n",
    "    curr = current state (tag)\n",
    "    returns q(curr|prev)\n",
    "    '''\n",
    "    def getTransition(self, prev, curr):\n",
    "        return self.transitions[prev][curr]\n",
    "\n",
    "    '''\n",
    "    This method accepts:\n",
    "        sentences - list of sentences, each represented as a list of words\n",
    "        sentences_tags - tags (as #s) corresponding to the words in sentences\n",
    "        tag_list - list of unique tags\n",
    "        n_prev - # of previous tags to consider. 2 for trigram, 1 for bigram\n",
    "    This calculates the transitions and emissions and stores them in the member variables\n",
    "    '''\n",
    "    def process(self, sentences, sentences_tags, tag_list, n_prev):\n",
    "        self.num_tags = len(tag_list)\n",
    "        self.__calculate_transitions(sentences_tags, self.num_tags, n_prev)\n",
    "        self.__calculate_emissions(sentences, sentences_tags, self.num_tags)\n",
    "        return\n",
    "\n",
    "    '''\n",
    "    Given the tags for each sentence and the number of unique tags,\n",
    "    this method returns a dictionary where key = tag and value = tag's count\n",
    "    '''\n",
    "    def __calculate_tag_counts(self, sentences_tags, num_tags):\n",
    "        tag_counts = [0] * num_tags\n",
    "        for sent_tags in sentences_tags:\n",
    "            for tag in sent_tags:\n",
    "                tag_counts[tag] +=1\n",
    "        return tag_counts\n",
    "\n",
    "\n",
    "    def handleLowFrequencyWords(self, e):\n",
    "        return e\n",
    "    \n",
    "    def divideLowFrequencyWords(self, tag_counts):\n",
    "        return\n",
    "\n",
    "    def getStatesList(self):\n",
    "        return list(q.keys())\n",
    "\n",
    "    '''\n",
    "    This method, called in process, calculates the emissions,\n",
    "    structured as a dictionary where each key is a word and it's value is a numpy array with a value for each tag.\n",
    "    To get e(word|tag), you do e[word][tag]\n",
    "    Given: list of sentences, corresponding tags, and number of unique tags\n",
    "    Emissions are calculated as the log((count(word, tag) + 1) / (count(tag) + dictionary_size)) - using add-1 smoothing\n",
    "    '''\n",
    "    def __calculate_emissions(self, sentences, sentences_tags, num_tags):\n",
    "        e = {}\n",
    "        # DIEGO: This can be included within the first loop which would save an iteration over the entire set of tags.\n",
    "        tag_counts = self.__calculate_tag_counts(sentences_tags, num_tags) # get dictionary of tag counts\n",
    "\n",
    "        # First go through all words/labels and count all (word,label) pairs\n",
    "        for sent, sent_tags in itertools.izip(sentences, sentences_tags):\n",
    "            for word, label in itertools.izip(sent, sent_tags):\n",
    "                if not word in e:\n",
    "                    e[word] = np.zeros(num_tags)\n",
    "                e[word][label] +=1\n",
    "\n",
    "        'This method makes more sense when the class is extended'\n",
    "        e = self.handleLowFrequencyWords(e)\n",
    "\n",
    "        dict_size = len(e)\n",
    "\n",
    "        # Divide the counts(word,tag) by count(tag) and log it\n",
    "        for i in range(num_tags):\n",
    "            for word, value in e.iteritems():\n",
    "                 # We compute values using the log and also use add 1 smoothing\n",
    "                e[word][i] = np.log( (e[word][i] + 1) / (tag_counts[i] + dict_size))\n",
    "\n",
    "        # NOTE: STILL NEED TO TAKE CARE OF UNKNOWN WORDS BY DOING THE FREQUENCY THING!!!\n",
    "        self.divideLowFrequencyWords(tag_counts)\n",
    "        \n",
    "        del tag_counts\n",
    "        self.emissions = e\n",
    "        return\n",
    "\n",
    "    '''\n",
    "    This method, called in process, calculates the transitions,\n",
    "    structured as a dictionary where each key is a previous state and it's value is a numpy array with a value for each tag.\n",
    "    To get q(tag|prev_state), you do transitions[prev_state][tag]\n",
    "    Prev_state is the previous tag in the bigram case, and previous 2 tags joined with ',' in trigram case\n",
    "    Given:\n",
    "        sentences_tags - list of tags separated by sentence,\n",
    "        num_tags - number of unique tags\n",
    "        n - number of previous states to use. 2 for trigram, 1 for bigrram\n",
    "    Transitions are calculated as the log((count(prev_state, tag) + 1) / (count(prev_state) + num_tags)) - using add-1 smoothing\n",
    "    '''\n",
    "    def __calculate_transitions(self, sentences_tags, num_tags, n):\n",
    "        q = {}\n",
    "        for state in itertools.product(range(-1,num_tags), repeat = n):\n",
    "            q[','.join(str(tag) for tag in state)] = np.zeros(num_tags)\n",
    "\n",
    "        for sent_tags in sentences_tags:  # for each sentence's list of tags\n",
    "            sent_tags = ([-1] * n) + sent_tags  #appending n '-1' tags to correspond to prior states/tags for the first word(s).\n",
    "\n",
    "            for i in range(len(sent_tags) - n):\n",
    "                prior_state = ','.join(str(sent_tags[j]) for j in range(i, i + n)) # Create a string with n sequenced tags\n",
    "                current_state = sent_tags[i + n]\n",
    "                q[prior_state][current_state] += 1\n",
    "\n",
    "        for prior_state, tag_list in q.iteritems():\n",
    "            q[prior_state] = np.log( (tag_list + 1) / (np.sum(tag_list) + num_tags) )\n",
    "\n",
    "        self.transitions = q\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class DictLowFreqClass(Dictionary):\n",
    "\n",
    "    def __init__(self, threshold = 7):\n",
    "        super(Dictionary, self).__init__()\n",
    "        self.classes = [\"twoDigitNum\", \"containsDigitAndAlpha\", \"initCap\", \"other\"]\n",
    "        self.regexs = [re.compile(r\"(\\d\\d)\"), re.compile(r\"^(\\w+-\\w+)\"), re.compile(r\"^([A-Z]\\w+)$\"), re.compile(r\"(.*)\")]\n",
    "        self.unknowns = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "    '''\n",
    "    This method runs over the dictionary generated with the training set, traces\n",
    "    the words with the lowest frequences (less than 5). There will be a class\n",
    "    used for words that can't fit any of the classes proposed. We take the words\n",
    "    and add the vectors in the dictionary to the corresponding class in the\n",
    "    unknown dictionary.\n",
    "    '''\n",
    "    def handleLowFrequencyWords(self, e):\n",
    "        # Initialize the dictionary with the classes\n",
    "        self.unknowns = {className: [0] * self.num_tags for className in self.classes}\n",
    "\n",
    "        for word in e.keys():\n",
    "            # Get the words with low counts\n",
    "            if e[word].sum() < self.threshold:\n",
    "                # Add them to the new dictionary\n",
    "                self.unknowns[self.getClassForWord(word)] += e[word]\n",
    "                # Remove them form the dictionary\n",
    "                del e[word]\n",
    "    \n",
    "        return e\n",
    "\n",
    "    def divideLowFrequencyWords(self, tag_counts):\n",
    "        # Divide the counts(word,tag) by count(tag) and log it\n",
    "        dict_size = len(self.unknowns)\n",
    "        for i in range(self.num_tags):\n",
    "            for word, value in self.unknowns.iteritems():\n",
    "                 # We compute values using the log and also use add 1 smoothing\n",
    "                self.unknowns[word][i] = np.log( (self.unknowns[word][i] + 1) / (tag_counts[i] + dict_size))\n",
    "    \n",
    "    def getClassForWord(self, word):\n",
    "        for className, regExpCode in itertools.izip(self.classes, self.regexs):\n",
    "            if regExpCode.search(word):\n",
    "#                 if className == \"other\":\n",
    "#                     print(word)\n",
    "                return className\n",
    "        return \"other\"\n",
    "\n",
    "    def getEmissionsByWord(self, word):\n",
    "        if self.isWordInDictionary(word):\n",
    "            return self.emissions[word]\n",
    "            #return super(Dictionary,self).getEmissionsByWord(word)\n",
    "        else:\n",
    "            return self.unknowns[self.getClassForWord(word)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [['i', 'went','to','i'],['to','Diego'],['love', 'i']]\n",
    "sentences_tags = [[1,0,2,1],[2,3],[4,1]]\n",
    "num_tags = 5\n",
    "tag_list = [0,1,2,3,4]\n",
    "n_prev = 2 # 2 is for trigram. 1 is for bigram\n",
    "\n",
    "d.process(sentences, sentences_tags, tag_list, n_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getEmissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getEmissionsByWord('Diego')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getTransitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getTransition('1,0', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: TAKE CARE OF UNKNOWN WORDS AND SMOOTHING \n",
    "(FIX VITERBI TO GET RID OF -1s)\n",
    "Fix question that I wrote in calculate_emissions method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VITERBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = ['i', 'went','to','i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#VITERBI\n",
    "n = 2 # trigram\n",
    "pis = [1] * num_tags\n",
    "paths = [[-1]*n] * num_tags\n",
    "\n",
    "for word in sentence:\n",
    "    e = d.getEmissionsByWord(word)\n",
    "    new_pis = [-float(\"inf\")] * num_tags\n",
    "    new_paths = [[]] * num_tags\n",
    "    for tag in range(num_tags):\n",
    "        e_pi = e[tag]\n",
    "        for prev_pi, prev_path in itertools.izip(pis, paths):\n",
    "            prev_state = ','.join(str(prev_path[j]) for j in range(-n, 0))\n",
    "            q_pi = d.getTransition(prev_state, tag)\n",
    "            pi = e_pi + prev_pi + q_pi\n",
    "            if pi > new_pis[tag]:\n",
    "                new_pis[tag] = pi\n",
    "                new_paths[tag] = prev_path + [tag]\n",
    "            \n",
    "    pis = new_pis\n",
    "    paths = new_paths\n",
    "\n",
    "\n",
    "best_path = paths[np.argmax(np.array(pis))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = Dictionary()\n",
    "d.process(train_x, train_y, tag_list, n_prev = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2 = DictLowFreqClass()\n",
    "d2.process(train_x, train_y, tag_list, n_prev = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list[20] # the greates number in e for 'the' was in index 10, which is 'DT' --> makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.getTransitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(sentence, d, num_tags = 45, n = 2):\n",
    "\n",
    "    pis = [1] * num_tags\n",
    "    paths = [[-1]*n] * num_tags\n",
    "\n",
    "    for word in sentence:\n",
    "        e = d.getEmissionsByWord(word)\n",
    "        new_pis = [-float(\"inf\")] * num_tags\n",
    "        new_paths = [[]] * num_tags\n",
    "        for tag in range(num_tags):\n",
    "            e_pi = e[tag]\n",
    "            for prev_pi, prev_path in itertools.izip(pis, paths):\n",
    "                prev_state = ','.join(str(prev_path[j]) for j in range(-n, 0))\n",
    "                q_pi = d.getTransition(prev_state, tag)\n",
    "                pi = e_pi + prev_pi + q_pi\n",
    "                if pi > new_pis[tag]:\n",
    "                    new_pis[tag] = pi\n",
    "                    new_paths[tag] = prev_path + [tag]\n",
    "\n",
    "        pis = new_pis\n",
    "        paths = new_paths\n",
    "\n",
    "\n",
    "    best_path = paths[np.argmax(np.array(pis))]\n",
    "    return best_path[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing0\n",
      "Processing1\n",
      "Processing2\n",
      "Processing3\n",
      "Processing4\n",
      "Processing5\n",
      "Processing6\n",
      "Processing7\n",
      "Processing8\n",
      "Processing9\n",
      "Processing10\n",
      "Processing11\n",
      "Processing12\n",
      "Processing13\n",
      "Processing14\n",
      "Processing15\n",
      "Processing16\n",
      "Processing17\n",
      "Processing18\n",
      "Processing19\n",
      "Processing20\n",
      "Processing21\n",
      "Processing22\n",
      "Processing23\n",
      "Processing24\n",
      "Processing25\n",
      "Processing26\n",
      "Processing27\n",
      "Processing28\n",
      "Processing29\n",
      "Processing30\n",
      "Processing31\n",
      "Processing32\n",
      "Processing33\n",
      "Processing34\n",
      "Processing35\n",
      "Processing36\n",
      "Processing37\n",
      "Processing38\n",
      "Processing39\n",
      "Processing40\n",
      "Processing41\n",
      "Processing42\n",
      "Processing43\n",
      "Processing44\n",
      "Processing45\n",
      "Processing46\n",
      "Processing47\n",
      "Processing48\n",
      "Processing49\n",
      "Processing50\n",
      "Processing51\n",
      "Processing52\n",
      "Processing53\n",
      "Processing54\n",
      "Processing55\n",
      "Processing56\n",
      "Processing57\n",
      "Processing58\n",
      "Processing59\n",
      "Processing60\n",
      "Processing61\n",
      "Processing62\n",
      "Processing63\n",
      "Processing64\n",
      "Processing65\n",
      "Processing66\n",
      "Processing67\n",
      "Processing68\n",
      "Processing69\n",
      "Processing70\n",
      "Processing71\n",
      "Processing72\n",
      "Processing73\n",
      "Processing74\n",
      "Processing75\n",
      "Processing76\n",
      "Processing77\n",
      "Processing78\n",
      "Processing79\n",
      "Processing80\n",
      "Processing81\n",
      "Processing82\n",
      "Processing83\n",
      "Processing84\n",
      "Processing85\n",
      "Processing86\n",
      "Processing87\n",
      "Processing88\n",
      "Processing89\n",
      "Processing90\n",
      "Processing91\n",
      "Processing92\n",
      "Processing93\n",
      "Processing94\n",
      "Processing95\n",
      "Processing96\n",
      "Processing97\n",
      "Processing98\n",
      "Processing99\n",
      "Processing100\n",
      "Processing101\n",
      "Processing102\n",
      "Processing103\n",
      "Processing104\n",
      "Processing105\n",
      "Processing106\n",
      "Processing107\n",
      "Processing108\n",
      "Processing109\n",
      "Processing110\n",
      "Processing111\n",
      "Processing112\n",
      "Processing113\n",
      "Processing114\n",
      "Processing115\n",
      "Processing116\n",
      "Processing117\n",
      "Processing118\n",
      "Processing119\n",
      "Processing120\n",
      "Processing121\n",
      "Processing122\n",
      "Processing123\n",
      "Processing124\n",
      "Processing125\n",
      "Processing126\n",
      "Processing127\n",
      "Processing128\n",
      "Processing129\n",
      "Processing130\n",
      "Processing131\n",
      "Processing132\n",
      "Processing133\n",
      "Processing134\n",
      "Processing135\n",
      "Processing136\n",
      "Processing137\n",
      "Processing138\n",
      "Processing139\n",
      "Processing140\n",
      "Processing141\n",
      "Processing142\n",
      "Processing143\n",
      "Processing144\n",
      "Processing145\n",
      "Processing146\n",
      "Processing147\n",
      "Processing148\n",
      "Processing149\n",
      "Processing150\n",
      "Processing151\n",
      "Processing152\n",
      "Processing153\n",
      "Processing154\n",
      "Processing155\n",
      "Processing156\n",
      "Processing157\n",
      "Processing158\n",
      "Processing159\n",
      "Processing160\n",
      "Processing161\n",
      "Processing162\n",
      "Processing163\n",
      "Processing164\n",
      "Processing165\n",
      "Processing166\n",
      "Processing167\n",
      "Processing168\n",
      "Processing169\n",
      "Processing170\n",
      "Processing171\n",
      "Processing172\n",
      "Processing173\n",
      "Processing174\n",
      "Processing175\n",
      "Processing176\n",
      "Processing177\n",
      "Processing178\n",
      "Processing179\n",
      "Processing180\n",
      "Processing181\n",
      "Processing182\n",
      "Processing183\n",
      "Processing184\n",
      "Processing185\n",
      "Processing186\n",
      "Processing187\n",
      "Processing188\n",
      "Processing189\n",
      "Processing190\n",
      "Processing191\n",
      "Processing192\n",
      "Processing193\n",
      "Processing194\n",
      "Processing195\n",
      "Processing196\n",
      "Processing197\n",
      "Processing198\n",
      "Processing199\n",
      "Processing200\n",
      "Processing201\n",
      "Processing202\n",
      "Processing203\n",
      "Processing204\n",
      "Processing205\n",
      "Processing206\n",
      "Processing207\n",
      "Processing208\n",
      "Processing209\n",
      "Processing210\n",
      "Processing211\n",
      "Processing212\n",
      "Processing213\n",
      "Processing214\n",
      "Processing215\n",
      "Processing216\n",
      "Processing217\n",
      "Processing218\n",
      "Processing219\n",
      "Processing220\n",
      "Processing221\n",
      "Processing222\n",
      "Processing223\n",
      "Processing224\n",
      "Processing225\n",
      "Processing226\n",
      "Processing227\n",
      "Processing228\n",
      "Processing229\n",
      "Processing230\n",
      "Processing231\n",
      "Processing232\n",
      "Processing233\n",
      "Processing234\n",
      "Processing235\n",
      "Processing236\n",
      "Processing237\n",
      "Processing238\n",
      "Processing239\n",
      "Processing240\n",
      "Processing241\n",
      "Processing242\n",
      "Processing243\n",
      "Processing244\n",
      "Processing245\n",
      "Processing246\n",
      "Processing247\n",
      "Processing248\n",
      "Processing249\n",
      "Processing250\n",
      "Processing251\n",
      "Processing252\n",
      "Processing253\n",
      "Processing254\n",
      "Processing255\n",
      "Processing256\n",
      "Processing257\n",
      "Processing258\n",
      "Processing259\n",
      "Processing260\n",
      "Processing261\n",
      "Processing262\n",
      "Processing263\n",
      "Processing264\n",
      "Processing265\n",
      "Processing266\n",
      "Processing267\n",
      "Processing268\n",
      "Processing269\n",
      "Processing270\n",
      "Processing271\n",
      "Processing272\n",
      "Processing273\n",
      "Processing274\n",
      "Processing275\n",
      "Processing276\n",
      "Processing277\n",
      "Processing278\n",
      "Processing279\n",
      "Processing280\n",
      "Processing281\n",
      "Processing282\n",
      "Processing283\n",
      "Processing284\n",
      "Processing285\n",
      "Processing286\n",
      "Processing287\n",
      "Processing288\n",
      "Processing289\n",
      "Processing290\n",
      "Processing291\n",
      "Processing292\n",
      "Processing293\n",
      "Processing294\n",
      "Processing295\n",
      "Processing296\n",
      "Processing297\n",
      "Processing298\n",
      "Processing299\n",
      "Processing300\n",
      "Processing301\n",
      "Processing302\n",
      "Processing303\n",
      "Processing304\n",
      "Processing305\n",
      "Processing306\n",
      "Processing307\n",
      "Processing308\n",
      "Processing309\n",
      "Processing310\n",
      "Processing311\n",
      "Processing312\n",
      "Processing313\n",
      "Processing314\n",
      "Processing315\n",
      "Processing316\n",
      "Processing317\n",
      "Processing318\n",
      "Processing319\n",
      "Processing320\n",
      "Processing321\n",
      "Processing322\n",
      "Processing323\n",
      "Processing324\n",
      "Processing325\n",
      "Processing326\n",
      "Processing327\n",
      "Processing328\n",
      "Processing329\n",
      "Processing330\n",
      "Processing331\n",
      "Processing332\n",
      "Processing333\n",
      "Processing334\n",
      "Processing335\n",
      "Processing336\n",
      "Processing337\n",
      "Processing338\n",
      "Processing339\n",
      "Processing340\n",
      "Processing341\n",
      "Processing342\n",
      "Processing343\n",
      "Processing344\n",
      "Processing345\n",
      "Processing346\n",
      "Processing347\n",
      "Processing348\n",
      "Processing349\n",
      "Processing350\n",
      "Processing351\n",
      "Processing352\n",
      "Processing353\n",
      "Processing354\n",
      "Processing355\n",
      "Processing356\n",
      "Processing357\n",
      "Processing358\n",
      "Processing359\n",
      "Processing360\n",
      "Processing361\n",
      "Processing362\n",
      "Processing363\n",
      "Processing364\n",
      "Processing365\n",
      "Processing366\n",
      "Processing367\n",
      "Processing368\n",
      "Processing369\n",
      "Processing370\n",
      "Processing371\n",
      "Processing372\n",
      "Processing373\n",
      "Processing374\n",
      "Processing375\n",
      "Processing376\n",
      "Processing377\n",
      "Processing378\n",
      "Processing379\n",
      "Processing380\n",
      "Processing381\n",
      "Processing382\n",
      "Processing383\n",
      "Processing384\n",
      "Processing385\n",
      "Processing386\n",
      "Processing387\n",
      "Processing388\n",
      "Processing389\n",
      "Processing390\n",
      "Processing391\n",
      "Processing392\n",
      "Processing393\n",
      "Processing394\n",
      "Processing395\n",
      "Processing396\n",
      "Processing397\n",
      "Processing398\n",
      "Processing399\n",
      "Processing400\n",
      "Processing401\n",
      "Processing402\n",
      "Processing403\n",
      "Processing404\n",
      "Processing405\n",
      "Processing406\n",
      "Processing407\n",
      "Processing408\n",
      "Processing409\n",
      "Processing410\n",
      "Processing411\n",
      "Processing412\n",
      "Processing413\n",
      "Processing414\n",
      "Processing415\n",
      "Processing416\n",
      "Processing417\n",
      "Processing418\n",
      "Processing419\n",
      "Processing420\n",
      "Processing421\n",
      "Processing422\n",
      "Processing423\n",
      "Processing424\n",
      "Processing425\n",
      "Processing426\n",
      "Processing427\n",
      "Processing428\n",
      "Processing429\n",
      "Processing430\n",
      "Processing431\n",
      "Processing432\n",
      "Processing433\n",
      "Processing434\n",
      "Processing435\n",
      "Processing436\n",
      "Processing437\n",
      "Processing438\n",
      "Processing439\n",
      "Processing440\n",
      "Processing441\n",
      "Processing442\n",
      "Processing443\n",
      "Processing444\n",
      "Processing445\n",
      "Processing446\n",
      "Processing447\n",
      "Processing448\n",
      "Processing449\n",
      "Processing450\n",
      "Processing451\n",
      "Processing452\n",
      "Processing453\n",
      "Processing454\n",
      "Processing455\n",
      "Processing456\n",
      "Processing457\n",
      "Processing458\n",
      "Processing459\n",
      "Processing460\n",
      "Processing461\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for j in range(len(dev_x)):\n",
    "    print(\"Processing: \"+str(j))\n",
    "    path = viterbi(dev_x[j], d2)\n",
    "    output.append(np.sum(path[i] == dev_y[j][i] for i in range(len(path)))/float(len(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devset\n",
    "---\n",
    "trigram, unknown classes with threshold=7, viterbi, 0.93166619623302527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93166619623302527"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0\n",
      "Processing: 1\n",
      "Processing: 2\n",
      "Processing: 3\n",
      "Processing: 4\n",
      "Processing: 5\n",
      "Processing: 6\n",
      "Processing: 7\n",
      "Processing: 8\n",
      "Processing: 9\n",
      "Processing: 10\n",
      "Processing: 11\n",
      "Processing: 12\n",
      "Processing: 13\n",
      "Processing: 14\n",
      "Processing: 15\n",
      "Processing: 16\n",
      "Processing: 17\n",
      "Processing: 18\n",
      "Processing: 19\n",
      "Processing: 20\n",
      "Processing: 21\n",
      "Processing: 22\n",
      "Processing: 23\n",
      "Processing: 24\n",
      "Processing: 25\n",
      "Processing: 26\n",
      "Processing: 27\n",
      "Processing: 28\n",
      "Processing: 29\n",
      "Processing: 30\n",
      "Processing: 31\n",
      "Processing: 32\n",
      "Processing: 33\n",
      "Processing: 34\n",
      "Processing: 35\n",
      "Processing: 36\n",
      "Processing: 37\n",
      "Processing: 38\n",
      "Processing: 39\n",
      "Processing: 40\n",
      "Processing: 41\n",
      "Processing: 42\n",
      "Processing: 43\n",
      "Processing: 44\n",
      "Processing: 45\n",
      "Processing: 46\n",
      "Processing: 47\n",
      "Processing: 48\n",
      "Processing: 49\n",
      "Processing: 50\n",
      "Processing: 51\n",
      "Processing: 52\n",
      "Processing: 53\n",
      "Processing: 54\n",
      "Processing: 55\n",
      "Processing: 56\n",
      "Processing: 57\n",
      "Processing: 58\n",
      "Processing: 59\n",
      "Processing: 60\n",
      "Processing: 61\n",
      "Processing: 62\n",
      "Processing: 63\n",
      "Processing: 64\n",
      "Processing: 65\n",
      "Processing: 66\n",
      "Processing: 67\n",
      "Processing: 68\n",
      "Processing: 69\n",
      "Processing: 70\n",
      "Processing: 71\n",
      "Processing: 72\n",
      "Processing: 73\n",
      "Processing: 74\n",
      "Processing: 75\n",
      "Processing: 76\n",
      "Processing: 77\n",
      "Processing: 78\n",
      "Processing: 79\n",
      "Processing: 80\n",
      "Processing: 81\n",
      "Processing: 82\n",
      "Processing: 83\n",
      "Processing: 84\n",
      "Processing: 85\n",
      "Processing: 86\n",
      "Processing: 87\n",
      "Processing: 88\n",
      "Processing: 89\n",
      "Processing: 90\n",
      "Processing: 91\n",
      "Processing: 92\n",
      "Processing: 93\n",
      "Processing: 94\n",
      "Processing: 95\n",
      "Processing: 96\n",
      "Processing: 97\n",
      "Processing: 98\n",
      "Processing: 99\n",
      "Processing: 100\n",
      "Processing: 101\n",
      "Processing: 102\n",
      "Processing: 103\n",
      "Processing: 104\n",
      "Processing: 105\n",
      "Processing: 106\n",
      "Processing: 107\n",
      "Processing: 108\n",
      "Processing: 109\n",
      "Processing: 110\n",
      "Processing: 111\n",
      "Processing: 112\n",
      "Processing: 113\n",
      "Processing: 114\n",
      "Processing: 115\n",
      "Processing: 116\n",
      "Processing: 117\n",
      "Processing: 118\n",
      "Processing: 119\n",
      "Processing: 120\n",
      "Processing: 121\n",
      "Processing: 122\n",
      "Processing: 123\n",
      "Processing: 124\n",
      "Processing: 125\n",
      "Processing: 126\n",
      "Processing: 127\n",
      "Processing: 128\n",
      "Processing: 129\n",
      "Processing: 130\n",
      "Processing: 131\n",
      "Processing: 132\n",
      "Processing: 133\n",
      "Processing: 134\n",
      "Processing: 135\n",
      "Processing: 136\n",
      "Processing: 137\n",
      "Processing: 138\n",
      "Processing: 139\n",
      "Processing: 140\n",
      "Processing: 141\n",
      "Processing: 142\n",
      "Processing: 143\n",
      "Processing: 144\n",
      "Processing: 145\n",
      "Processing: 146\n",
      "Processing: 147\n",
      "Processing: 148\n",
      "Processing: 149\n",
      "Processing: 150\n",
      "Processing: 151\n",
      "Processing: 152\n",
      "Processing: 153\n",
      "Processing: 154\n",
      "Processing: 155\n",
      "Processing: 156\n",
      "Processing: 157\n",
      "Processing: 158\n",
      "Processing: 159\n",
      "Processing: 160\n",
      "Processing: 161\n",
      "Processing: 162\n",
      "Processing: 163\n",
      "Processing: 164\n",
      "Processing: 165\n",
      "Processing: 166\n",
      "Processing: 167\n",
      "Processing: 168\n",
      "Processing: 169\n",
      "Processing: 170\n",
      "Processing: 171\n",
      "Processing: 172\n",
      "Processing: 173\n",
      "Processing: 174\n",
      "Processing: 175\n",
      "Processing: 176\n",
      "Processing: 177\n",
      "Processing: 178\n",
      "Processing: 179\n",
      "Processing: 180\n",
      "Processing: 181\n",
      "Processing: 182\n",
      "Processing: 183\n",
      "Processing: 184\n",
      "Processing: 185\n",
      "Processing: 186\n",
      "Processing: 187\n",
      "Processing: 188\n",
      "Processing: 189\n",
      "Processing: 190\n",
      "Processing: 191\n",
      "Processing: 192\n",
      "Processing: 193\n",
      "Processing: 194\n",
      "Processing: 195\n",
      "Processing: 196\n",
      "Processing: 197\n",
      "Processing: 198\n",
      "Processing: 199\n",
      "Processing: 200\n",
      "Processing: 201\n",
      "Processing: 202\n",
      "Processing: 203\n",
      "Processing: 204\n",
      "Processing: 205\n",
      "Processing: 206\n",
      "Processing: 207\n",
      "Processing: 208\n",
      "Processing: 209\n",
      "Processing: 210\n",
      "Processing: 211\n",
      "Processing: 212\n",
      "Processing: 213\n",
      "Processing: 214\n",
      "Processing: 215\n",
      "Processing: 216\n",
      "Processing: 217\n",
      "Processing: 218\n",
      "Processing: 219\n",
      "Processing: 220\n",
      "Processing: 221\n",
      "Processing: 222\n",
      "Processing: 223\n",
      "Processing: 224\n",
      "Processing: 225\n",
      "Processing: 226\n",
      "Processing: 227\n",
      "Processing: 228\n",
      "Processing: 229\n",
      "Processing: 230\n",
      "Processing: 231\n",
      "Processing: 232\n",
      "Processing: 233\n",
      "Processing: 234\n",
      "Processing: 235\n",
      "Processing: 236\n",
      "Processing: 237\n",
      "Processing: 238\n",
      "Processing: 239\n",
      "Processing: 240\n",
      "Processing: 241\n",
      "Processing: 242\n",
      "Processing: 243\n",
      "Processing: 244\n",
      "Processing: 245\n",
      "Processing: 246\n",
      "Processing: 247\n",
      "Processing: 248\n",
      "Processing: 249\n",
      "Processing: 250\n",
      "Processing: 251\n",
      "Processing: 252\n",
      "Processing: 253\n",
      "Processing: 254\n",
      "Processing: 255\n",
      "Processing: 256\n",
      "Processing: 257\n",
      "Processing: 258\n",
      "Processing: 259\n",
      "Processing: 260\n",
      "Processing: 261\n",
      "Processing: 262\n",
      "Processing: 263\n",
      "Processing: 264\n",
      "Processing: 265\n",
      "Processing: 266\n",
      "Processing: 267\n",
      "Processing: 268\n",
      "Processing: 269\n",
      "Processing: 270\n",
      "Processing: 271\n",
      "Processing: 272\n",
      "Processing: 273\n",
      "Processing: 274\n",
      "Processing: 275\n",
      "Processing: 276\n",
      "Processing: 277\n",
      "Processing: 278\n",
      "Processing: 279\n",
      "Processing: 280\n",
      "Processing: 281\n",
      "Processing: 282\n",
      "Processing: 283\n",
      "Processing: 284\n",
      "Processing: 285\n",
      "Processing: 286\n",
      "Processing: 287\n",
      "Processing: 288\n",
      "Processing: 289\n",
      "Processing: 290\n",
      "Processing: 291\n",
      "Processing: 292\n",
      "Processing: 293\n",
      "Processing: 294\n",
      "Processing: 295\n",
      "Processing: 296\n",
      "Processing: 297\n",
      "Processing: 298\n",
      "Processing: 299\n",
      "Processing: 300\n",
      "Processing: 301\n",
      "Processing: 302\n",
      "Processing: 303\n",
      "Processing: 304\n",
      "Processing: 305\n",
      "Processing: 306\n",
      "Processing: 307\n",
      "Processing: 308\n",
      "Processing: 309\n",
      "Processing: 310\n",
      "Processing: 311\n",
      "Processing: 312\n",
      "Processing: 313\n",
      "Processing: 314\n",
      "Processing: 315\n",
      "Processing: 316\n",
      "Processing: 317\n",
      "Processing: 318\n",
      "Processing: 319\n",
      "Processing: 320\n",
      "Processing: 321\n",
      "Processing: 322\n",
      "Processing: 323\n",
      "Processing: 324\n",
      "Processing: 325\n",
      "Processing: 326\n",
      "Processing: 327\n",
      "Processing: 328\n",
      "Processing: 329\n",
      "Processing: 330\n",
      "Processing: 331\n",
      "Processing: 332\n",
      "Processing: 333\n",
      "Processing: 334\n",
      "Processing: 335\n",
      "Processing: 336\n",
      "Processing: 337\n",
      "Processing: 338\n",
      "Processing: 339\n",
      "Processing: 340\n",
      "Processing: 341\n",
      "Processing: 342\n",
      "Processing: 343\n",
      "Processing: 344\n",
      "Processing: 345\n",
      "Processing: 346\n",
      "Processing: 347\n",
      "Processing: 348\n",
      "Processing: 349\n",
      "Processing: 350\n",
      "Processing: 351\n",
      "Processing: 352\n",
      "Processing: 353\n",
      "Processing: 354\n",
      "Processing: 355\n",
      "Processing: 356\n",
      "Processing: 357\n",
      "Processing: 358\n",
      "Processing: 359\n",
      "Processing: 360\n",
      "Processing: 361\n",
      "Processing: 362\n",
      "Processing: 363\n",
      "Processing: 364\n",
      "Processing: 365\n",
      "Processing: 366\n",
      "Processing: 367\n",
      "Processing: 368\n",
      "Processing: 369\n",
      "Processing: 370\n",
      "Processing: 371\n",
      "Processing: 372\n",
      "Processing: 373\n",
      "Processing: 374\n",
      "Processing: 375\n",
      "Processing: 376\n",
      "Processing: 377\n",
      "Processing: 378\n",
      "Processing: 379\n",
      "Processing: 380\n",
      "Processing: 381\n",
      "Processing: 382\n",
      "Processing: 383\n",
      "Processing: 384\n",
      "Processing: 385\n",
      "Processing: 386\n",
      "Processing: 387\n",
      "Processing: 388\n",
      "Processing: 389\n",
      "Processing: 390\n",
      "Processing: 391\n",
      "Processing: 392\n",
      "Processing: 393\n",
      "Processing: 394\n",
      "Processing: 395\n",
      "Processing: 396\n",
      "Processing: 397\n",
      "Processing: 398\n",
      "Processing: 399\n",
      "Processing: 400\n",
      "Processing: 401\n",
      "Processing: 402\n",
      "Processing: 403\n",
      "Processing: 404\n",
      "Processing: 405\n",
      "Processing: 406\n",
      "Processing: 407\n",
      "Processing: 408\n",
      "Processing: 409\n",
      "Processing: 410\n",
      "Processing: 411\n",
      "Processing: 412\n",
      "Processing: 413\n",
      "Processing: 414\n",
      "Processing: 415\n",
      "Processing: 416\n",
      "Processing: 417\n",
      "Processing: 418\n",
      "Processing: 419\n",
      "Processing: 420\n",
      "Processing: 421\n",
      "Processing: 422\n",
      "Processing: 423\n",
      "Processing: 424\n",
      "Processing: 425\n",
      "Processing: 426\n",
      "Processing: 427\n",
      "Processing: 428\n",
      "Processing: 429\n",
      "Processing: 430\n",
      "Processing: 431\n",
      "Processing: 432\n",
      "Processing: 433\n",
      "Processing: 434\n",
      "Processing: 435\n",
      "Processing: 436\n",
      "Processing: 437\n",
      "Processing: 438\n",
      "Processing: 439\n",
      "Processing: 440\n",
      "Processing: 441\n",
      "Processing: 442\n",
      "Processing: 443\n",
      "Processing: 444\n",
      "Processing: 445\n",
      "Processing: 446\n",
      "Processing: 447\n",
      "Processing: 448\n",
      "Processing: 449\n",
      "Processing: 450\n",
      "Processing: 451\n",
      "Processing: 452\n",
      "Processing: 453\n",
      "Processing: 454\n",
      "Processing: 455\n",
      "Processing: 456\n",
      "Processing: 457\n",
      "Processing: 458\n",
      "Processing: 459\n",
      "Processing: 460\n",
      "Processing: 461\n",
      "Processing: 462\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for j in range(len(test_x)):\n",
    "    print(\"Processing: \"+str(j))\n",
    "    path = viterbi(test_x[j], d2)\n",
    "    output.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('output.csv','w')\n",
    "f.write(\"id,tag\\n\")\n",
    "count = 0\n",
    "for sentence in output:\n",
    "    for word in sentence:\n",
    "        f.write(str(count) + \",\\\"\" + tag_list[word] + \"\\\"\\n\")\n",
    "        count += 1\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
